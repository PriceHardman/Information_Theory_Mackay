{
 "metadata": {
  "name": "",
  "signature": "sha256:11c27e94558c5cfa6442ff3875448c9cb6666932e44144dd43e8cea9fd73403b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Chapter 2: Probability, Entropy, and Inference"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2.1: Probabilities and ensembles"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Ensembles"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "An **ensemble** $X$ is a triple $\\left(x,\\mathcal{A}x,\\mathcal{P}x\\right)$, where the *outcome* $x$ is the value of a random variable $\\mathcal{A}x$ taking on a set of possible values $\\mathcal{A}x = \\left\\{a_{1},a_{2},\\cdots,a_{i},\\cdots,a_{I}\\right\\}$, with probabilities $\\mathcal{P}x = \\left\\{p_{1},p_{2},\\cdots,p_{I}\\right\\}$, where $P\\left(x=a_{i}\\right) = p_{i}$, $p_{i} \\geq 0$, and $\\sum_{a_{i} \\in \\mathcal{A}x}{P\\left(x=a_{i}\\right)} = 1$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "The probability of a subset"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If $T$ is a subset of $\\mathcal{A}x$, then\n",
      "$$P\\left(T\\right) = P\\left(x \\in T\\right) = \\sum_{a_{i}\\in T}{P\\left(x=a_{i}\\right)}$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Joint Ensembles"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A joint ensemble $XY$ is an ensemble in which each outcome is an ordered pair $x,y$ with $x \\in \\mathcal{A}x = \\left\\{a_{1},\\cdots,a_{I}\\right\\}$ and $y \\in \\mathcal{A}y = \\left\\{b_{1},\\cdots,b_{I}\\right\\}$. We call $P\\left(x,y\\right)$ the **joint probability** of $x$ and $y$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Marginal Probability"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can obtain the marginal probability $P\\left(x\\right)$ from the joint probability $P\\left(x,y\\right)$ by summation over all $y \\in \\mathcal{A}y$:\n",
      "$$P\\left(x\\right) = P\\left(x=a_{i}\\right) = \\sum_{y\\in\\mathcal{A}y}{P\\left(x = a_{i},y\\right)}$$ In other words, we sum over all the $y$'s for which $x=a_{i}$, thereby obtaining a measure of the probability of $x$ irrespective of $y$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Conditional Probability"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$P\\left(x=a_{i}|y=b_{j}\\right) = \\frac{P\\left(x=a_{i},y=b_{j}\\right)}{P\\left(y=b_{j}\\right)}, P\\left(y=b_{j}\\right) \\neq 0$$\n",
      "Intuitively, the conditional probability of $x$ given $y$ is equal to the proportion of instances of $y$ for which $x$ also occurs."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "The Product Rule"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given a set of assumptions $\\mathcal{H}$ upon which some probabilities are based, the product rule tells us that\n",
      "$$P\\left(x,y|\\mathcal{H}\\right) = P\\left(x|y,\\mathcal{H}\\right)P\\left(y|\\mathcal{H}\\right) = \n",
      "P\\left(y|x,\\mathcal{H}\\right)P\\left(x|\\mathcal{H}\\right)\n",
      "$$\n",
      "This is otherwise known as the *chain rule*, and it follows from the definition of conditional probability."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Sum Rule"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "By rewriting the definition of marginal probability, we find that\n",
      "\\begin{align}\n",
      "P\\left(x|\\mathcal{H}\\right) &= \\sum_{y}{P\\left(x,y|\\mathcal{H}\\right)} \\\\\n",
      "&= \\sum_{y}{P\\left(x|y, \\mathcal{H}\\right)P\\left(y|\\mathcal{H}\\right)}\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Bayes' Theorem"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Obtained by rearranging the second the third terms in the definition of the product rule, and then incorporating the sum rule, we find that\n",
      "\\begin{align}\n",
      "P\\left(y|x,\\mathcal{H}\\right) &= \n",
      "\\frac{P\\left(x|y,\\mathcal{H}\\right)P\\left(y|\\mathcal{H}\\right)}{P\\left(x|\\mathcal{H}\\right)} \\\\\n",
      "&= \\frac{P\\left(x|y,\\mathcal{H}\\right)P\\left(y|\\mathcal{H}\\right)}{\\sum_{y'}{P\\left(x|y',\\mathcal{H}\\right)P\\left(y'|\\mathcal{H}\\right)}}\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Independence"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Two random variables $X$ and $Y$ are independent if and only if\n",
      "$$P\\left(x,y\\right) = P\\left(x\\right)P\\left(y\\right)$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Example 2.3:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Jo has a test for a nasty disease. We denote Jo's state of health by the variable $a$ and the test results by $b$.\n",
      "\\begin{align}\n",
      "a=1 \\Rightarrow & \\text{    Jo has the disease} \\\\\n",
      "a=0 \\Rightarrow & \\text{    Jo does not have the disease}\n",
      "\\end{align}\n",
      "The result of the test is either 'positive' ($b=1$) or 'negative' ($b=0$). The test is 95% reliable: In 95% of cases of people with the disease, a positive result is returned, and in 95% of cases of people who don't have it, a negative result is returned. The final piece of background information is that 1% of people Jo's age and background have the disease. Jo has the test and the result is positive. What is the probability that she has the disease?\n",
      "**<br/><br/>\n",
      "**Solution**: Write down all the provided probabilities. The test reliability specifies the conditional probability of $b$ given $a$, or\n",
      "\\begin{array}{cc}\n",
      "P\\left(b=1|a=1\\right) = 0.95 & P\\left(b=1|a=0\\right) = 0.05 \\\\\n",
      "P\\left(b=0|a=1\\right) = 0.05 & P\\left(b=0|a=0\\right) = 0.95 \\\\\n",
      "\\end{array}\n",
      "and the  disease prevalence tells us about the marginal probability of $a$:\n",
      "$$P\\left(a=1\\right) = 0.01, \\;\\;\\;\\;\\; P\\left(a=0\\right) = 0.99$$\n",
      "From the marginal $P\\left(a\\right)$ and the conditional $P\\left(b|a\\right)$, we can deduce the joint probability $P\\left(a,b\\right) = P\\left(a\\right)P\\left(b|a\\right)$ Given that Jo's test has come up positive ($b=1$), we use Bayes' theorem to deduce\n",
      "\\begin{align}\n",
      "P\\left(a=1|b=1\\right) &= \n",
      "\\frac{P\\left(b=1|a=1\\right)P\\left(a=1\\right)}{P\\left(b=1\\right)} \\\\\n",
      "&= \\frac{P\\left(b=1|a=1\\right)P\\left(a=1\\right)}{P\\left(b=1|a=1\\right)P\\left(a=1\\right) + P\\left(b=1|a=0\\right)P\\left(a=0\\right)} \\\\\n",
      "&= \\frac{0.95 \\times 0.01}{0.95 \\times 0.01 + 0.05 \\times 0.99} \\\\\n",
      "&= 0.16\n",
      "\\end{align}\n",
      "So in spite of the positive result, the probability is only 16% that Jo actually has the disease.\n",
      "$\\Box$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2.2: The Meaning of Probability"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Probabilities can be used in two ways.<br/>\n",
      "Firstly, they can describe **frequencies of outcomes in random experiments**, but using this framework it is difficult to give noncircular definitions to the terms \"frequency\" and \"random\". For instance, what does it mean to say that the frequency of a tossed coin coming up heads is $\\frac{1}{2}$? If we say the frequency is the average fraction of heads in long sequences, we need to define \"average\", which is difficult without using a word synonymous with probability.<br/>\n",
      "More generally, probabilities can used to describe **degrees of belief** in propositions that do not involve random variables. For example, 'The probability that Mr. S. murdered Mrs. S, given the evidence' is such a proposition. While the layperson is content to use both interpretations of probability interchangeably, classical statistics refers to probability exclusively in terms of frequencies of outcomes in repeatable random experiments.<br/>\n",
      "Nevertheless, degrees of belief *can* in fact be mapped onto probabilities if they satisfy a simple set of consistency rules known as the **Cox axioms**:\n",
      "1. **Degrees of belief can be ordered.** Let $B\\left(x\\right)$ denote the degree of belief in somne proposition $x$. If $B\\left(x\\right)$ is greater than $B\\left(y\\right)$ and $B\\left(y\\right)$ is greater than $B\\left(z\\right)$, then $B\\left(x\\right)$ is greater than $B\\left(z\\right)$. This allows for beliefs to be mapped onto $\\mathbb{R}$.\n",
      "2. **There is a relation between the degree of belief in proposition, and that of its negation.** Let $\\overline{x}$ be the negation of some proposition $x$. Then there is a function $f$ such that \n",
      "$$B\\left(x\\right) = f\\left(B\\left(\\overline{x}\\right)\\right)$$\n",
      "3. **There is a relation between the degree of belief the conjunctional proposition $x\\cap y$, the degree of belief in the conditional proposition $x|y$, and the degree of belief in the proposition $y$**. That is, there exists a function $g$ such that \n",
      "$$B\\left(x,y\\right) = g\\left[B\\left(x|y\\right),B\\left(y\\right)\\right].$$\n",
      "<br/><br/>\n",
      "In general, this use of probability to quantify degrees of belief is known as the **Bayesian viewpoint**, or the subjective interpretation of probability, since it relies upon assumptions. Advocates of the Bayesian approach to data modeling and pattern recognition do not view this subjectivity as a defect, but rather believe that one cannot do inference without making assumptions. The big difference between the frequentist and the Bayesian approaches is that Bayesians also use probabilities to describe inferences."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2.3: Forward Probabilities and Inverse Probabilities"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Probability calculations fall into two categories: *forward probabilities* and *inverse probabilities*. Here is an example of a forward probability problem:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise 2.4: An urn contains $K$ balls, of which $B$ are black and $W=K-B$ are white. Fred draws a ball at random from the urn and replaces it. He does this $N$ times.What is the probability distribution of the number of times a black ball is drawn, $n_{B}$?**\n",
      "<br/>\n",
      "Since Fred is replacing the balls each time, each draw is independent with the same probability $\\frac{B}{K}$, and there are only two outcomes, the number of times a black ball is drawn as a binomial distribution. For $N$ trials and constant $p_{B} = \\frac{B}{K}$,\n",
      "$$P\\left(n_{B}|p_{B},N\\right) = {N \\choose n_{B}}\\left(p_{B}\\right)^{n_{B}}\\left(1-p_{B}\\right)^{N-n_{B}}$$\n",
      "<br/>$\\Box$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Forward probability problems involve a *generative model* that describes a process that is assumed to give rise to some data; the task is to compute the probability distribution or expectation of some quantity that depends on the data.<br/><br/>\n",
      "While forward probability problems require that we calculate the probability distribution of some quantity *produced* by the process, in inverse probability problems we compute the conditional probability of one or more of the *unobserved variables* in the process, *given* the observed variables. To see the distinction, consider the following example of an inverse probability problem."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Example 2.6: There are eleven urns labeled by $u \\in \\left\\{0,1,2,\\cdots,10\\right\\}$, each containing ten balls. Urn $u$ contains $u$ black balls and $10-u$ white balls. Fred selects an urn $u$ at random and draws $N$ times with replacement from that urn, obtaining $n_{B}$ black balls and $N-n_{N}$ white ones. Fred's friend Bill looks on. If after $N=10$ draws Fred has drawn $n_{B}=3$ black balls, what is the probability that the urn Fred is using is urn $u$, assuming Bill doesn't know the value of $u$?**<br/>\n",
      "The joint probability distribution of $u$ and $n_{B}$ can be written as \n",
      "$$P\\left(u,n_{B}|N\\right) = P\\left(n_{B}|u,N\\right)P\\left(u\\right)$$\n",
      "From this we can obtain the conditional distribution of $u$ given $n_{B}$:\n",
      "\\begin{align}\n",
      "P\\left(u|n_{B},N\\right) &= \n",
      "\\frac{P\\left(u,n_{B}|N\\right)}{P\\left(n_{B}|N\\right)}\\\\\n",
      "&= \\frac{P\\left(n_{B}|u,N\\right)P\\left(u\\right)}{P\\left(n_{B}|N\\right)}\n",
      "\\end{align}\n",
      "We know the three values on the right. The marginal probability $P\\left(u\\right)$ is simply $\\frac{1}{11}$, since the urn is picked at random out of 11. The chance of drawing a black ball from urn $u$ is $f_{u} = \\frac{u}{10}$, so $$P\\left(n_{b}|u,N\\right) = {N \\choose n_{b}}\\left(f_{u}\\right)^{n_{B}}\\left(1-f_{u}\\right)^{N-n_{B}}$$\n",
      "The denominator, $P\\left(n_{b}|N\\right)$ is the marginal probability of $n_{b}$, which can be obtained by the sum rule:\n",
      "$$P\\left(n_{b}|N\\right) = \n",
      "\\sum_{u}{P\\left(u,n_{b}|N\\right)} = \n",
      "\\sum_{u}{P\\left(n_{b}|u,N\\right)P\\left(u\\right)}$$\n",
      "So our conditional probability of $u$ given $n_{B}$ is:\n",
      "\\begin{align}\n",
      "P\\left(u|n_{B},N\\right)\n",
      "&= \\frac{P\\left(n_{B}|u,N\\right)P\\left(u\\right)}{P\\left(n_{B}|N\\right)} \\\\\n",
      "&= \\frac{1}{\\sum_{u}{P\\left(n_{b}|u,N\\right)P\\left(u\\right)}}\\frac{1}{11}{N \\choose n_{b}}\\left(f_{u}\\right)^{n_{B}}\\left(1-f_{u}\\right)^{N-n_{B}}\n",
      "\\end{align}\n",
      "Knowing that $N=10$ and $n_{b} = 3$, we can conclude\n",
      "$$\n",
      "P\\left(u|n_{B}=3,N=10\\right) \n",
      "= \\frac{1}{\\sum_{u}{P\\left(3|u,10\\right)P\\left(u\\right)}}\\frac{1}{11}{10 \\choose 3}\\left(f_{u}\\right)^{3}\\left(1-f_{u}\\right)^{10-3}\n",
      "$$\n",
      "From this point it is simply plug and chug:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from scipy import misc\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "def P_nb_given_N(nb=3,N=10):\n",
      "    P_u = 1.0/11.0\n",
      "    return sum([\n",
      "        P_u*misc.comb(N,nb)*(u/10.0)**nb*(1-(u/10.0))**(N-nb) for u in np.arange(11)\n",
      "    ])\n",
      "\n",
      "def P_u_given_nb_N(u,nb=3,N=10):\n",
      "    fu = u/10.0\n",
      "    Pu = 1.0/11.0\n",
      "    return (1/P_nb_given_N(nb,N))*Pu*misc.comb(N,nb)*fu**nb*(1-fu)**(N-nb)\n",
      "\n",
      "u = np.arange(11)\n",
      "P = P_u_given_nb_N(u,nb=3,N=10)\n",
      "plt.bar(u,P,align='center')\n",
      "plt.xlabel(r'$u$',fontsize=20)\n",
      "plt.ylabel(r'$P\\left(u|n_{B},N\\right)$',fontsize=20)\n",
      "plt.xticks(u)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEYCAYAAAB7twADAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noEiSmjBQJElNGCiSpCb+P4avsoHQD+mHAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7600e80>"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The probability that Fred drew out of urn $u$ is given above, and we can see that the most likely such urn was $u=3$, with a posterior probability of around $0.3$.$\\Box$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Terminology of Inverse Probabilities"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If $\\mathbf{\\theta}$ denotes unknown parameters, $D$ denotes data, and $\\mathcal{H}$ denotes the overall hypothesis space, the general equation\n",
      "$$P\\left(\\mathbf{\\theta}|D,\\mathcal{H}\\right) = \n",
      "\\frac{P\\left(D|\\mathbf{\\theta},\\mathcal{H}\\right)P\\left(\\mathbf{\\theta}|\\mathcal{H}\\right)}{P\\left(D|\\mathcal{H}\\right)}$$ is written:\n",
      "$$\\text{posterior} = \\frac{\\text{likelihood}\\times\\text{prior}}{\\text{evidence}},$$\n",
      "where the **likelihood** $P\\left(D|\\mathbf{\\theta},\\mathcal{H}\\right)$ represents the probability of observing our data, given parameters $\\mathbf{\\theta}$. Take note that here the data $D$ is fixed (to what was observed), and the parameter $\\mathbf{\\theta}$ is allowed to vary. The **prior** $P\\left(\\mathbf{\\theta}|\\mathcal{H}\\right)$ represents the uncertainty about $\\mathbf{\\theta}$ before any data is considered. The **evidence** $P\\left(D|\\mathcal{H}\\right)$, sometimes known as the **marginal likelihood**, represents the probability of our data irrespective of any model with parameters $\\mathbf{\\theta}$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Inverse Probability and Prediction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Example 2.6: Assuming again that Bill has observed $n_{B}=3$ blacks in $N=10$ draws, let Fred draw another ball from the same urn. What is the probability that the next drawn ball is black?**<br/>\n",
      "By the sum rule, we find that\n",
      "\\begin{align}\n",
      "P\\left(\\text{ball}_{N+1}\\text{ black}|n_{B},N\\right) &= \\sum_{u}{P\\left(\\text{ball}_{N+1}\\text{ black}|u,n_{B},N\\right)P\\left(u|n_{b},N\\right)}\\\\\n",
      "&= \\sum_{u}{f_{u}P\\left(u|n_{B},N\\right)},\n",
      "\\end{align}\n",
      "since $P\\left(\\text{ball}_{N+1}\\text{ black}|u,n_{B},N\\right)$ is simply the probability of drawing a black ball from urn $u$, which is $f_{u}$.\n",
      "Using the values for $P\\left(u|n_{B},N\\right)$ that we already calculated, we find that:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from scipy import misc\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "def P_nb_given_N(nb=3,N=10):\n",
      "    P_u = 1.0/11.0\n",
      "    return sum([\n",
      "        P_u*misc.comb(N,nb)*(u/10.0)**nb*(1-(u/10.0))**(N-nb) for u in np.arange(11)\n",
      "    ])\n",
      "\n",
      "def P_u_given_nb_N(u,nb=3,N=10):\n",
      "    fu = u/10.0\n",
      "    Pu = 1.0/11.0\n",
      "    return (1/P_nb_given_N(nb,N))*Pu*misc.comb(N,nb)*fu**nb*(1-fu)**(N-nb)\n",
      "\n",
      "u = np.arange(11)\n",
      "fu = u/10.0\n",
      "P = P_u_given_nb_N(u,nb=3,N=10)\n",
      "print(\"P(ball N+1 is black|nb=3,N=10) = %0.2f\" % (sum(fu*P)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "P(ball N+1 is black|nb=3,N=10) = 0.33\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Thus,\n",
      "\\begin{align}\n",
      "P\\left(\\text{ball}_{N+1}\\text{ black}|n_{B},N\\right) &= \\sum_{u}{P\\left(\\text{ball}_{N+1}\\text{ black}|u,n_{B},N\\right)P\\left(u|n_{b},N\\right)}\\\\\n",
      "&= \\sum_{u}{f_{u}P\\left(u|n_{B},N\\right)} \\\\\n",
      "&= 0.333.\n",
      "\\end{align}\n",
      "$\\Box$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice how this result differs from the frequentist approach. Classical statistics would dictate that we first choose the most plausible hypothesis before making our prediction. In this case, that would mean that we would first set $u=3$, since we already calculated that urn $u=3$ is the most likely urn from which Fred drew the ball. Since the probability of drawing a black ball from urn $u=3$ is $f_{3} = \\frac{3}{10} = 0.3$, we would have obtained a slightly different answer had we adopted this approach. Instead, we have accounted for our uncertainty about the urn from which the ball was drawn by marginalizing over $u$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Inference as Inverse Probability"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Consider the following:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Example 2.7: Bill tosses a bent coing $N$ times, obtaining a sequence of heads and tails. We assume the coin has a probability $f_{H}$ of coming up heads, but we don't know $f_{H}$. In $n_{H}$ heads have occurred in $N$ tosses, what is the probability distribution of $f_{H}$?**<br/>\n",
      "Unlike example 2.6, there's a subjective element here. Given a restricted definition of probability that says \"probabilities are the frequencies of random variables\", this example is different from the urn problems. Whereas the urn $u$ was a random variable, the bias $f_{H}$ of the coin would not normally be called a random variable. It is just a fixed but unknown parameter in which we are interested. Despite this significant conceptual difference, the problems nevertheless bear striking similarities to one another. To solve this problem, we have to make assumption about what the bias of the coin $f_{H}$ might be. This prior probability distribution over $f_{H}$, $P\\left(f_{H}\\right)$, corresponds to the prior over $u$ in the elevent urns problems. In real life, we have to make assumptions in order to assign priors; those assumptions will be subjective, and our answers will depend on them. Exactly the same can be said for the other probabilities in our generative model too. We are assuming, for example, that balls are drawn independently, but could there not be correlations in Fred's ball-drawing action? Indeed there could be, so the likelihood function that we used depended on assumptions in that case also. In real data modeling problems, priors are subjective, *and so are likelihoods*. We are now using $P\\left(\\right)$ to denote probability densities over continuous variables, as well as probabilities over discrete variables and probabilities of logical propositions. The probability that a continuous variable $v$ lies between values $a$ and $b$ (where $b > a$) is defined to be $\\int_{a}^{b}{P\\left(v\\right)dv}$. $P\\left(v\\right)dv$ is dimensionless. Don't be surprised to see probability densities greater than 1. This is normal and nothing is wrong as long as $\\int_{a}^{b}{P\\left(v\\right)dv} \\leq 1$ for any interval $\\left(a,b\\right)$. We can now reformulate our question:<br/>\n",
      "**Assuming a uniform prior on $f_{H}$, $P\\left(f_{H}\\right)=1$, solve the problem posed in example 2.7. Sketch the posterior distribution of $f_{H}$ and compute the probability that the $N+1$th outcome will be heads. The beta integral will prove useful: $\\int_{0}^{1}{x^{a}\\left(1-x\\right)^{b}dx} = \\frac{a!\\cdot b!}{\\left(a+b+1\\right)!}$**<br/>\n",
      "By Bayes' Theorem, we know that\n",
      "\\begin{align}\n",
      "P\\left(f_{H}|n_{H},N\\right) &= \\frac{P\\left(n_{H}|f_{H},N\\right)P\\left(f_{H}\\right)}{P\\left(n_{H}|N\\right)} \\\\\n",
      "&= \\frac{P\\left(n_{H}|f_{H},N\\right)\\left(1\\right)}{\\int_{0}^{1}{P\\left(n_{H}|f_{H},N\\right)P\\left(f_{H}\\right)df_{H}}} ,\\;\\;\\; \\text{by sum rule and that }P\\left(f_{H}\\right)=1 \\\\\n",
      "&= f_{H}^{n_{H}}\\left(1-f_{H}\\right)^{N-n_{H}} \\frac{\\left(N+1\\right)!}{n_{H}!\\left(N-n_{H}\\right)!} ,\\;\\;\\; \\text{by the beta integral}\n",
      "\\end{align}\n",
      "Let $h$ be the event that the $\\left(N+1\\right)$'st toss comes up heads. To calculate the probability of $h$, we need only integrate (i.e. marginalize) over $f_{H}$ and substitute in the above expression:\n",
      "\\begin{align}\n",
      "P\\left(h|n_{H},N\\right) &= \\int_{0}^{1}{P\\left(h|f_{H},n_{H},N\\right)P\\left(f_{H}|n_{H},N\\right)df_{H}} \\\\\n",
      "&= \\int_{0}^{1}{f_{H}P\\left(f_{H}|n_{H},N\\right)df_{H}},\\;\\;\\; \\text{ as }P\\left(h|f_{H},n_{H},N\\right)=f_{H}. \\\\\n",
      "&= \\frac{\\left(N+1\\right)!}{n_{H}!\\left(N-n_{H}\\right)!} \\int_{0}^{1}{f_{H}^{n_{H}+1}\\left(1-f_{H}\\right)^{N-n_{H}}df_{H}} \\\\\n",
      "&= \\frac{\\left(N+1\\right)!}{n_{H}!\\left(N-n_{H}\\right)!} \\cdot \\frac{\\left(n_{H}+1\\right)!\\left(N-n_{h}\\right)!}{\\left(N+2\\right)!} \\\\\n",
      "&= \\frac{N_{h}+1}{N+2}\n",
      "\\end{align}$\\Box$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "People sometimes confuse assigning a prior distribution to an unknown parameter such as $f_{H}$ with making an initial guess as to the *value* of the parameter. But the prior over $f_{H}$, $P\\left(f_{H}\\right)$, is not a simple statement like \"Initially, I would guess $f_{H}=\\frac{1}{2}$.\" The prior is a probability density over $f_{H}$ which specifies the prior degree of belief that $f_{H}$ lies in any interval $\\left(f,f+\\delta f\\right)$. It may well be the case that our prior for $f_{H}$ is symmetric about $\\frac{1}{2}$, so that the *mean* of $f_{H}$ under the prior is $\\frac{1}{2}$. In this case, the predictive distribution for the first toss $x_{1}$ would be\n",
      "$$P\\left(x_{1}=\\text{head}\\right) = \n",
      "\\int{P\\left(f_{H}\\right)P\\left(x_{1} = \\text{head}|f_{H}\\right)df_{H}} = \n",
      "\\int{P\\left(f_{H}\\right)f_{H}df_{H}} = \\frac{1}{2}$$\n",
      "However, the prediction for subsequent tosses will depend on the whole prior distribution, not just its mean."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Data Compression and Inverse Probability"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Consider the following task:<br/>\n",
      "**Write a computer program capable of compressing binary files like this one:\n",
      "**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "***000000000000000000000000000010000000100000000000000000000000000000100\n",
      "000000000100000000000000000000000100000000000000000000000010000000000\n",
      "000000000000000010000000000000000000100000000000000001000000000000100\n",
      "001000000000000000000000000000000000000000000100000000000000000000000\n",
      "000000000000000001000000000010000000100000000000000000100000000000100\n",
      "000001000000000000000000000000000000000000000000100000000000000100000***"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Intuitively, compression works by taking advantage of the predictability of a file. In this case, the source of the file appears more likely to emit zeros than ones. A data compression program that compresses this file must, implicitly or explicitly, be addressing the question: \"What is the probability that the next character in this file is a 1?\""
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "The Likelihood Principle"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Solve the following two exercises:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Example 2.10: Urn $A$ contains 3 balls: one black, and two white. Urn $B$ contains three balls as well: two black and one white. One of the urns is selected at random and one ball is drawn. The ball is black. What is the probability the selected urn is urn $A$?**<br/>\n",
      "Let $u$ be the selected urn, and let $b$ be the event that the drawn ball is black. Since we picked the urn at random, we find that\n",
      "\\begin{align}\n",
      "P\\left(u=A|b\\right) &= \\frac{P\\left(b|u=A\\right)P\\left(u=A\\right)}{P\\left(b\\right)} \\\\\n",
      "&= \\frac{P\\left(b|u=A\\right)P\\left(u=A\\right)}{\\sum_{u}{P\\left(b|u\\right)P\\left(u\\right)}} \\\\\n",
      "&= \\frac{\\frac{1}{3}\\cdot\\frac{1}{2}}{\\frac{1}{3}\\cdot\\frac{1}{2}+\\frac{2}{3}\\cdot\\frac{1}{2}} \\\\\n",
      "&= \\frac{1}{6}\\cdot\\frac{2}{1} \\\\\n",
      "&= \\frac{1}{3}\n",
      "\\end{align}$\\Box$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Example 2.11: Urn $A$ contains five balls: one black, two white, one green, and one pink. Urn $B$ contains five hundred balls: two hundred black, one hundred white, 50 yellow, 40 cyan, 30 sienna, 25 green, 25 silver, 20 gold, and 10 purple. Note that one fifth of $A$'s balls are black, and two fifths of $B$'s are black. One of the urns is selected at random and one ball is drawn. The ball is black. What is the probability that the urn is urn $A$?**<br/>\n",
      "Again, let $u$ be the selected urn, and let $b$ be the event that the drawn ball is black. Since we picked the urn at random, we find that\n",
      "\\begin{align}\n",
      "P\\left(u=A|b\\right) &= \\frac{P\\left(b|u=A\\right)P\\left(u=A\\right)}{P\\left(b\\right)} \\\\\n",
      "&= \\frac{P\\left(b|u=A\\right)P\\left(u=A\\right)}{\\sum_{u}{P\\left(b|u\\right)P\\left(u\\right)}} \\\\\n",
      "&= \\frac{\\frac{1}{5}\\cdot\\frac{1}{2}}{\\frac{1}{5}\\cdot\\frac{1}{2}+\\frac{2}{5}\\cdot\\frac{1}{2}} \\\\\n",
      "&= \\frac{1}{10}\\cdot\\frac{10}{3} \\\\\n",
      "&= \\frac{1}{3}\n",
      "\\end{align}$\\Box$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What do you notice about the two solutions? Does each answer depend on the detailed contents of each urn? No, the details of the other possible outcomes and their probabilities are irrelevant. All that matters is the probability of the outcome that actually happened (here, that the drawn ball is black) given the different hypotheses. We need only know the *likelihood*, i.e. how the probability of the data that happened varies with the hypothesis. This simple rule about inference is known as the *likelihood principle*:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**The Likelihood Principle**: Given a generative model for data $d$ given parameters $\\mathbf{\\theta}$, $P\\left(d|\\mathbf{\\theta}\\right)$, and having observed a particular outcome $d_{1}$, all inferences and predictions should depend only on the function $P\\left(d_{1}|\\mathbf{\\theta}\\right)$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2.4: Definition of Entropy and Related Functions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The **Shannon information content of an outcome** $x$ is defined to be\n",
      "$$h\\left(x\\right) = \\log_{2}\\frac{1}{P\\left(x\\right)}.$$\n",
      "It is measured in bits."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**The entropy of an ensemble** $X$ is defined to be the average Shannon information content of an outcome:\n",
      "$$H\\left(X\\right) = \\sum_{x\\in\\mathcal{A}x}{P\\left(x\\right)\\log\\frac{1}{P\\left(x\\right)}},$$\n",
      "with the convention that for $P\\left(x\\right)=0$ that $0\\times\\log\\frac{1}{0}=0$.<br/>\n",
      "Like information content, entropy is measured in bits. Another name for the entropy of $X$ is the uncertainty of $X$, as more uniformly distributed $X$'s have higher entropies.<br/><br/>\n",
      "We'll now note some properties of the entropy function:\n",
      "* $H\\left(X\\right) \\geq 0$ with equality iff $p_{i} = 1$ for one $i$.\n",
      "* Entropy is maximized if $\\mathbf{p}$ is uniform."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2.5: Decomposability of the Entropy"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The entropy function satisfies a recursive property that can be very useful when computing entropies. To illustrate, imagine a random variable $x \\in \\left\\{0,1,2\\right\\}$ is created by first flipping a fair coin to determine whether $x=0$; then, if $x\\neq 0$, flipping a fair coin a second time to determine whether $x$ is $1$ or $2$. The distribution of $x$ is\n",
      "$$P\\left(x=0\\right) = \\frac{1}{2};\\; P\\left(x=1\\right) = \\frac{1}{4};\\; P\\left(x=2\\right) = \\frac{1}{4}$$<br/>\n",
      "What is the entropy of $X$? We can compute it by brute force:\n",
      "$$H\\left(X\\right) = \\frac{1}{2}\\log2 + \\frac{1}{4}\\log4 + \\frac{1}{4}\\log4 = 1.5$$<br/>\n",
      "Alternatively, we can make use of the following decomposition. Imagine first learning whether $x=0$, and then, if $x \\neq 0$, learning which non-zero value is the case. The revelation of whether $x=0$ or not entails releaving a binary variable whose probability distribution is $\\left\\{\\frac{1}{2},\\frac{1}{2}\\right\\}$. This has an entropy of $H\\left(\\left\\{\\frac{1}{2},\\frac{1}{2}\\right\\}\\right) = \\frac{1}{2}\\log 2 + \\frac{1}{2}\\log 2 = 1$. If $x$ is not 0, we learn the value of the second coin flip. This too is a binary variable whose probability distribution is $\\left\\{\\frac{1}{2},\\frac{1}{2}\\right\\}$, and whose entropy is 1 bit. We only get to experience this second revelation half the time, however, so the entropy can be written\n",
      "$$H\\left(X\\right) = H\\left(\\left\\{\\frac{1}{2},\\frac{1}{2}\\right\\}\\right) + \\frac{1}{2}H\\left(\\left\\{\\frac{1}{2},\\frac{1}{2}\\right\\}\\right)$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2.6: Gibbs' Inequality"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The **relative entropy**, or Kullback-Leibler divergence, between two distributions $P\\left(x\\right)$ and $Q\\left(x\\right)$ defined over the same alphabet $\\mathcal{A}x$ is\n",
      "$$D_{KL}\\left(P||Q\\right) = \\sum_{x}{P\\left(x\\right)\\log\\frac{P\\left(x\\right)}{Q\\left(x\\right)}}$$<br/>\n",
      "The relative entropy satisfies **Gibbs' Inequality**\n",
      "$$D_{KL}\\left(P||Q\\right) \\geq 0$$\n",
      "with equality only if $P=Q$. Note that in general the relative entropy is not symmetric under interchange of $P$ and $Q$. Gibbs' inequality is very important, and will be referred to often. Gibbs' as well as many other inequalities, can be proved using the concept of convexity."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2.7: Jensen's Inequality for Convex Functions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The words 'convex $\\smile$' and 'concave $\\frown$' may be pronounced as 'convex-smile' and 'concave-frown'. This is a usefully redundant pnemonic, as we might otherwise forget the direction to which the terms convex and concave are in reference."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Convex $\\smile$ functions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A function $f\\left(x\\right)$ is convex $\\smile$ over $\\left(a,b\\right)$ if every chord of the function lies above the function. That is, $\\forall x_{1},x_{2} \\in \\left(a,b\\right)$ and $0 \\leq \\lambda \\leq 1$,\n",
      "$$f\\left(\\lambda x_{1} + \\left(1-\\lambda\\right)x_{2}\\right) < \\lambda f\\left(x_{1}\\right) + \\left(1-\\lambda\\right)f\\left(x_{2}\\right)$$\n",
      "A function $f$ is ** strictly convex $\\smile$ ** if $\\forall x_{1},x_{2} \\in \\left(a,b\\right)$, the equality holds only for $\\lambda = 0$ and $\\lambda = 1$. Similar defintions apply to concave $\\frown$ and strictly concave $\\frown$ functions.<br/><br/>\n",
      "An example of a strictly convex function is $x^{2}$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Jensen's Inequality"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If $f$ is a convex $\\smile$ function and $x$ is a random variable, then\n",
      "$$E\\left[f\\left(x\\right)\\right] \\geq f\\left(E\\left[x\\right]\\right),$$\n",
      "where $E$ denotes expected value: $$E\\left(X\\right) = \\sum_{x \\in X}{xP\\left(x\\right)}$$."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}